# 方言识别系统改进建议

## 问题分析

当前识别错误（四川话被识别为客家话）可能的原因：

### 1. 模型性能限制
- **准确率**: 78% → 22%错误率
- **训练数据**: 每类仅250张图片 → 数据量不足
- **模型大小**: EfficientNet-B0 → 最小的模型

### 2. 类别不平衡
- 某些方言样本可能质量更好
- 训练数据分布可能不均衡

### 3. 特征相似性
- 四川话和客家话在某些声学特征上可能相似
- 声谱图可能无法捕捉细微差异

## 短期改进方案

### 1. 数据增强
```python
# 在 train.py 中增加数据增强
transforms.Compose([
    transforms.RandomResizedCrop(size),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 新增
    transforms.RandomRotation(5),  # 新增
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
```

### 2. 增加训练数据
- 每类从250张增加到500-1000张
- 使用全部6000段录音

### 3. 调整训练参数
```python
# train.py
parser.add_argument('--epochs', type=int, default=50)  # 从30增加到50
parser.add_argument('--batch-size', type=int, default=16)  # 从2增加到16
parser.add_argument('--lr', type=float, default=0.001)  # 降低学习率
```

### 4. 使用更大的模型
```python
# 从 EfficientNet-B0 升级到 B3 或 B4
from model import efficientnet_b3 as create_model  # 或 efficientnet_b4
```

### 5. 类别权重平衡
```python
# 在训练时使用加权损失
from torch.nn import CrossEntropyLoss

# 计算每个类别的权重
class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)
criterion = CrossEntropyLoss(weight=torch.FloatTensor(class_weights))
```

## 中期改进方案

### 1. 改进音频特征提取
当前使用声谱图，可以尝试：
- **Mel频率倒谱系数 (MFCC)**
- **Mel声谱图** 而不是普通声谱图
- **Gammatone滤波器**

```python
import librosa

# 使用 Mel 声谱图
mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
```

### 2. 集成学习
训练多个模型并投票：
```python
# 使用不同的模型
models = [
    efficientnet_b0(num_classes=9),
    efficientnet_b2(num_classes=9),
    efficientnet_b4(num_classes=9)
]

# 投票决策
predictions = [model(x) for model in models]
final_pred = torch.mode(torch.stack(predictions), dim=0)
```

### 3. 混淆矩阵分析
添加代码查看哪些方言容易混淆：
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# 在验证时
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('预测')
plt.ylabel('实际')
plt.title('方言混淆矩阵')
plt.savefig('confusion_matrix.png')
```

## 长期改进方案

### 1. 使用专门的音频模型
- **Wav2Vec 2.0** (Facebook)
- **HuBERT** 
- **WavLM**
- **Whisper** (OpenAI) 的特征提取器

```python
from transformers import Wav2Vec2Model

model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
```

### 2. 端到端模型
不使用声谱图，直接处理音频：
```python
import torchaudio

# 直接从波形训练
class DialectCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # 1D卷积处理音频
        self.conv1 = nn.Conv1d(1, 64, kernel_size=80, stride=16)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)
        # ...
```

### 3. 注意力机制
添加注意力层关注重要的音频段：
```python
class AttentionLayer(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        weights = F.softmax(self.attention(x), dim=1)
        return torch.sum(x * weights, dim=1)
```

### 4. 多任务学习
同时预测方言和其他特征（性别、年龄）：
```python
class MultiTaskModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = efficientnet_b0()
        self.dialect_head = nn.Linear(1280, 9)
        self.gender_head = nn.Linear(1280, 2)
        self.age_head = nn.Linear(1280, 5)
```

## 数据改进

### 1. 数据清洗
- 检查并移除低质量音频
- 统一音频长度和采样率
- 去除静音段

### 2. 数据平衡
```python
from imblearn.over_sampling import SMOTE

# 对少数类别进行过采样
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)
```

### 3. 主动学习
- 找出模型不确定的样本
- 人工标注这些困难样本
- 重新训练

## 评估改进

### 1. 详细指标
不只看准确率，还要看：
```python
from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred, 
                          target_names=dialect_names))
# 输出每个类别的精确率、召回率、F1分数
```

### 2. Per-class 准确率
```python
# 查看每个方言的识别准确率
for i, dialect in enumerate(dialect_names):
    mask = y_true == i
    acc = (y_pred[mask] == i).sum() / mask.sum()
    print(f"{dialect}: {acc:.2%}")
```

## 快速验证

先用简单方法验证是否能改进：

### 测试1: 增加训练轮数
```bash
cd train&predict
python train.py --epochs 50 --batch-size 8
```

### 测试2: 使用更多数据
将每类从250张增加到500张

### 测试3: 使用B3模型
```python
# 修改 train.py
from model import efficientnet_b3 as create_model
num_model = "B3"
```

## 当前可以立即做的

1. **收集更多四川话样本进行测试**
2. **使用诊断工具检查音频质量**
3. **尝试不同的音频（不同说话人、不同内容）**
4. **查看训练日志，了解四川话的训练准确率**

## 预期改进效果

| 方案 | 预期准确率提升 | 实施难度 |
|------|--------------|---------|
| 增加数据 | +5-10% | 低 |
| 数据增强 | +2-5% | 低 |
| 更大模型 | +3-7% | 中 |
| Mel声谱图 | +3-5% | 中 |
| 端到端模型 | +5-15% | 高 |
| Wav2Vec2 | +10-20% | 高 |

## 总结

当前78%的准确率对于一个简单模型来说已经不错，但仍有很大改进空间。建议优先：

1. ✅ 增加训练数据（每类至少500张）
2. ✅ 训练更多epochs（50轮）
3. ✅ 使用更大的模型（B3或B4）
4. ✅ 添加数据增强
5. ✅ 分析混淆矩阵，针对性优化

这些改进预计可以将准确率提升到85-90%。


