import sys
sys.path.append('GUI')
from model import efficientnet_b4
import torch
from PIL import Image
from torchvision import transforms
import json
import numpy as np
import scipy.io.wavfile as wav
from numpy.lib import stride_tricks
from scipy import signal

def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):
    win = window(frameSize)
    hopSize = int(frameSize - np.floor(overlapFac * frameSize))
    samples = np.append(np.zeros(int(np.floor(frameSize/2.0))), sig)
    cols = int(np.ceil((len(samples) - frameSize) / float(hopSize)) + 1)
    samples = np.append(samples, np.zeros(frameSize))
    frames = stride_tricks.as_strided(samples, shape=(cols, frameSize), 
                                      strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()
    frames *= win
    return np.fft.rfft(frames)

def logscale_spec(spec, sr=44100, factor=20., alpha=1.0, f0=0.9, fmax=1):
    """è®­ç»ƒæ—¶ä½¿ç”¨çš„logscale_specç®—æ³•"""
    spec = spec[:, 0:256]
    timebins, freqbins = np.shape(spec)
    scale = np.linspace(0, 1, freqbins)
    scale = np.array(list(map(lambda x: x * alpha if x <= f0 else (fmax-alpha*f0)/(fmax-f0)*(x-f0)+alpha*f0, scale)))
    scale *= (freqbins-1)/max(scale)

    newspec = np.complex128(np.zeros([timebins, freqbins]))
    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])
    freqs = [0.0 for i in range(freqbins)]
    totw = [0.0 for i in range(freqbins)]
    
    for i in range(0, freqbins):
        if (i < 1 or i + 1 >= freqbins):
            newspec[:, i] += spec[:, i]
            freqs[i] += allfreqs[i]
            totw[i] += 1.0
            continue
        else:
            w_up = scale[i] - np.floor(scale[i])
            w_down = 1 - w_up
            j = int(np.floor(scale[i]))
            newspec[:, j] += w_down * spec[:, i]
            freqs[j] += w_down * allfreqs[i]
            totw[j] += w_down
            newspec[:, j + 1] += w_up * spec[:, i]
            freqs[j + 1] += w_up * allfreqs[i]
            totw[j + 1] += w_up
    
    for i in range(len(freqs)):
        if (totw[i] > 1e-6):
            freqs[i] /= totw[i]
    
    return newspec, freqs

def create_spectrogram_correct(audiopath):
    """ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹æ³•ç”Ÿæˆå£°è°±å›¾"""
    samplerate, samples = wav.read(audiopath)
    print(f"åŸå§‹é‡‡æ ·ç‡: {samplerate} Hz, æ—¶é•¿: {len(samples)/samplerate:.2f}ç§’")
    
    # é‡é‡‡æ ·åˆ°48000Hzï¼ˆå¦‚æœéœ€è¦ï¼‰
    target_sr = 48000
    if samplerate != target_sr:
        print(f"é‡é‡‡æ ·: {samplerate} Hz -> {target_sr} Hz")
        num_samples = int(len(samples) * target_sr / samplerate)
        samples = signal.resample(samples, num_samples)
        samplerate = target_sr
    
    # æˆªå–å‰5ç§’
    max_duration = 5
    if len(samples) > samplerate * max_duration:
        print(f"æˆªå–å‰{max_duration}ç§’")
        samples = samples[:int(samplerate * max_duration)]
    
    # å•å£°é“
    if len(samples.shape) > 1:
        samples = samples[:, 0]
    
    # ç”ŸæˆSTFT
    s = stft(samples, 1024)  # æ³¨æ„ï¼šè®­ç»ƒæ—¶ç”¨çš„æ˜¯1024ï¼Œä¸æ˜¯512ï¼
    
    # Logscaleå˜æ¢
    sshow, freq = logscale_spec(s, factor=1, sr=samplerate, alpha=1.0)
    sshow = sshow[2:, :]  # å»æ‰å‰2è¡Œ
    
    # è½¬æ¢ä¸ºåˆ†è´ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
    ims = 20. * np.log10(np.abs(sshow) / 10e-6)
    
    # è½¬ç½®ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
    ims = np.transpose(ims)
    
    # è£å‰ªé¢‘ç‡èŒƒå›´
    ims = ims[0:256, :]
    
    print(f"å£°è°±å›¾å°ºå¯¸: {ims.shape}")
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    image = Image.fromarray(ims)
    image = image.convert('L')
    
    # è½¬æ¢ä¸ºRGBï¼ˆæ¨¡å‹éœ€è¦3é€šé“ï¼‰
    image = image.convert('RGB')
    
    return image

def predict_with_correct_spectrogram(audio_path):
    # ç”Ÿæˆæ­£ç¡®çš„å£°è°±å›¾
    spectrogram = create_spectrogram_correct(audio_path)
    
    # æ•°æ®è½¬æ¢
    data_transform = transforms.Compose([
        transforms.Resize((380, 380)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    img = data_transform(spectrogram)
    img = torch.unsqueeze(img, dim=0)
    
    # åŠ è½½æ¨¡å‹
    device = torch.device("cpu")
    model = efficientnet_b4(num_classes=10).to(device)
    model.load_state_dict(torch.load('GUI/weight/model-29.pth', map_location=device))
    model.eval()
    
    # é¢„æµ‹
    with torch.no_grad():
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
    
    # åŠ è½½ç±»åˆ«
    with open('GUI/class_indices.json', 'r') as f:
        class_indict = json.load(f)
    
    dialect_names = {
        "changsha": "é•¿æ²™è¯",
        "hebei": "æ²³åŒ—è¯",
        "hefei": "åˆè‚¥è¯",
        "kejia": "å®¢å®¶è¯",
        "minnan": "é—½å—è¯",
        "nanchang": "å—æ˜Œè¯",
        "ningxia": "å®å¤è¯",
        "shan3xi": "é™•è¥¿è¯",
        "shanghai": "ä¸Šæµ·è¯",
        "sichuan": "å››å·è¯"
    }
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•éŸ³é¢‘: {audio_path.split('/')[-1]}")
    print(f"{'='*60}")
    
    sorted_indices = torch.argsort(predict, descending=True)
    for i, idx in enumerate(sorted_indices[:5]):
        class_name = class_indict[str(idx.item())]
        dialect_name = dialect_names.get(class_name, class_name)
        prob = predict[idx].item()
        marker = "ğŸ‘‘" if i == 0 else "  "
        print(f"{marker} {dialect_name:8s}: {prob:6.2%}")
    
    predict_cla = torch.argmax(predict).item()
    predicted_class = class_indict[str(predict_cla)]
    predicted_name = dialect_names.get(predicted_class, predicted_class)
    print(f"\nâœ“ è¯†åˆ«ç»“æœ: {predicted_name} (ç½®ä¿¡åº¦: {predict[predict_cla].item():.2%})")
    print(f"{'='*60}\n")

if __name__ == '__main__':
    print("="*60)
    print("ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„å£°è°±å›¾ç”Ÿæˆæ–¹å¼æµ‹è¯•")
    print("="*60)
    
    test_files = [
        '/Users/wangbo/Downloads/å››å·è¯æ ‡æ³¨æ ·ä¾‹/audio/recorder1238A.wav',
        '/Users/wangbo/Downloads/å››å·è¯æ ‡æ³¨æ ·ä¾‹/audio/recorder1239A.wav',
        '/Users/wangbo/Downloads/å››å·è¯æ ‡æ³¨æ ·ä¾‹/audio/recorder1240A.wav',
    ]
    
    for audio_file in test_files:
        try:
            predict_with_correct_spectrogram(audio_file)
        except Exception as e:
            print(f"å¤„ç† {audio_file} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

